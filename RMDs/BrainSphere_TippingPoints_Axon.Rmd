---
title: "Calculation of Toxicological Tipping Points from Boyd et al., 2025 BrainSphere Data-Axon Tracking"
author: "Sierra Boyd"
date: "2025-03-27"
output: html_document
---


```{r}
library(tidyverse)
library(dplyr)
library(tidyr)
library(readxl)
library(pracma)
library(conflicted)
library(data.table)
library(pheatmap)
library(caret)
library(umap)
library(randomForest)

conflicts_prefer(dplyr::filter())
conflicts_prefer(dplyr::select)
```


```{r}
#Load Activity Data
#indicate input data folder
project.input.dir <-"~"

#Make list of data files for network scan data
csv_files<- list.files(path=project.input.dir, pattern = "_Melt\\.csv$", recursive = T, full.names = T) 

#load data files and remove extra columns
for (i in csv_files){
  df<-fread(i)
  df<-df[,-1]
  full_data<-df
}

#remove any data with missing DOP values
full_data<-full_data[!is.na(DOP)]

#modify dates (the date should reflect the last recording date for all samples)
full_data<-full_data%>%
  group_by(Group_ID)%>%
  mutate(date=paste(date[DOP==29][1]))%>%
  ungroup()

#Check for duplicates and remove
full_data<-full_data%>%distinct(Endpoint,Chemical,Dosage,Group_ID,DOP,.keep_all=TRUE)

#remove the post-washout day from analysis
full_data<-full_data%>%
  filter(
    (DOP %in% c(15,20,22,27)))

#make a wide data fromat
full_data_wide<-full_data%>%
  pivot_wider(names_from=Endpoint,values_from=value)                       
                             
# Rename all zero dose wells to say Control                   
full_data_wide<-full_data_wide%>%mutate(Chemical = ifelse(Dosage == "Control", "Control", Chemical))     
full_data_wide<-full_data_wide%>%mutate(Dosage = ifelse(Chemical == "Control", 0, Dosage)) 
full_data_wide$Dosage<-as.numeric(full_data_wide$Dosage)

#update column and endpoint names                            
setnames(full_data_wide,"Chemical","trt")                          
setnames(full_data_wide,"DOP","DIV")                    
setnames(full_data_wide,"Dosage","dose")  

setnames(full_data_wide,"neuronConductionVel_mean_neuron","Neuron_Conduction_Velocity_Neuron")
setnames(full_data_wide,"neuronFiringRate_mean_neuron","Firing_Rate_Neuron")
setnames(full_data_wide,"totNoSpikes_mean_neuron","Number_of_Spikes_per_Neuron_Neuron")

#Add a units column
full_data_wide$units<-paste("uM")

#separate the assay tag
full_data_wide <- full_data_wide %>% separate(Group_ID, into = c("Group", "Plate.SN","well"), sep = "_")

#Indicate plate type. Plate numbers starting in T are twenty four well. M is for six well
full_data_wide<-full_data_wide%>%mutate(Plate_Type=ifelse(grepl("T","Plate.SN"),"Twenty_four","Six"))

#Remove the Plate.SN and Group columns
full_data_wide<-full_data_wide%>%select(-c("Plate.SN","Group"))

#update the Plate_Type column name
setnames(full_data_wide,"Plate_Type","Plate.SN")

#select only the columns of interest (identifers and endponits of interest)
full_data_wide<-full_data_wide%>%select(c("date","Plate.SN","DIV","well","trt","dose","units","Neuron_Conduction_Velocity_Neuron","Firing_Rate_Neuron","Number_of_Spikes_per_Neuron_Neuron"))


full_data_wide<-filter(full_data_wide, !is.na(Neuron_Conduction_Velocity_Neuron))

#set as a data frame
all_data<-as.data.frame(full_data_wide)

set_output <- "zscore_plots"  # Can be set to "zscore_plots" (plot individual parameters), "total_perturb" (look at scalar perturb and calculate velocity and critical concentration), or "rand_total_perturb"
set_normalization <- "plate" # Can be set to "global" or "plate"

#indicate parameters of interest (selected endpoints)
set_parameters <- c("Neuron_Conduction_Velocity_Neuron","Firing_Rate_Neuron","Number_of_Spikes_per_Neuron_Neuron") 

#save the wrangled data as a csv
write.csv(all_data,file = "~")

```


#Function 1 to normalize and scale plate
```{r}

## Apply normalization to the plate level by dividing raw values by plate median of controls only, then scaling with z-score across plates
## Computing z-score based on control well distribution only (i.e. the z-score reflects number of SD away from control median)
normalize_by_plate <- function(all_data) {
  
  # Split data by plate (date as well because plateIDs get reused) and DIV
  per_plate_split <- split(all_data, interaction(all_data[,"date"], all_data[,"Plate.SN"], all_data[,"DIV"], drop=TRUE))
  
  scaled_plates <- list() #initialize list
  
  # Loop through each plate at each DIV
  for (i in per_plate_split) {
    
    # Loop through each ontogeny parameter
    for (j in names(all_data[,8:(ncol(all_data))])) {
      
      cntrls <- (subset(i, trt=="Control")[,j]) # Get vector of control values for that DIV on that plate
      
      # If all plate values for a parameter are NA, set all fold-changes to 1
      if (length(na.omit(cntrls)) == 0) {
        print(paste("All NAs for", i[,"Plate.SN"][1], i[,"DIV"][1], j, sep=" - "))
        i[,j] <- 1
        
        # If plate median is still zero, set all fold-changes to 1
      } else if (median(na.omit(cntrls)) == 0) {
        print(paste("All Zeros for", i[,"Plate.SN"][1], i[,"DIV"][1], j, sep=" - "))
        i[,j] <- 1
        
        # If plate median is not zero, divide to get fold-change
      } else {
        i[,j] <- i[,j] / median(na.omit(cntrls)) # Divide all values on that plate by median of the plate
      }      
      
    }
    
    scaled_plates[[length(scaled_plates)+1]] <- i
  }
  
  rm(i,j)
  scaled_plates <- do.call(rbind, scaled_plates) # Re-form one table of values
  
  scaled_plates[is.na(scaled_plates)] <- 1 # Change all remaining NAs to 1
 # scaled_plates[scaled_plates[,"R"] < 0, "R"] <- 0 # Set correlation value minimums to zero
  
  # Log-transform fold-change values
  scaled_plates[,8:(ncol(scaled_plates)-1)] <- log2(scaled_plates[,8:(ncol(scaled_plates)-1)] + 1)
  
  
  ## Z-score scale each parameter over each DIV timepoint
  # First split by DIV
  div_split <- split(scaled_plates, scaled_plates[,"DIV"]) 
  
  zscore_plates <- list() #initialize list
  
  # Loop through DIV
  for (i in div_split) {
    
    # Loop through network params
    for (j in names(scaled_plates[,8:(ncol(scaled_plates)-1)])) {
      
      cntrl_vals <- subset(i, trt=="Control")[,j] # Isolate control values for that parameter
      i[,j] <- (i[,j] - mean(cntrl_vals)) / sd(cntrl_vals) # Calculate z-score
    }
    
    zscore_plates[[length(zscore_plates)+1]] <- i # Add to list
  }
  
  rm(i,j)
  zscore_plates <- do.call(rbind, zscore_plates) # Re-form one table of values
  
  zscore_plates
}
```

#Apply function 1 (Normalize by plate)
```{r}
z_norm_data <- normalize_by_plate(all_data)
```

#Function to plot
```{r}
## Separate out data for compound of interest and plot all network parameter median z-scores
## With cone of uncertainty (3 times the median absolute deviation)
plot_dose_perturbations <- function(norm_data, parameter, compound) {
  
  # Find median of replicate response on each DIV
  data_meds <- as.data.frame(do.call(cbind, aggregate(get(parameter) ~ trt + dose + DIV, FUN=function(x) c(med=median(x), mad=mad(x), n=length(x), low=(median(x) - (sd(x)/sqrt(length(x)))), high=(median(x) + (sd(x)/sqrt(length(x))))), data=norm_data)), stringsAsFactors=FALSE)
  
  # Subset to compound of interest and control values
  trt_meds <- subset(data_meds, trt %in% c(compound, "Control"))
  
  # Add in DIV0 placeholder data to anchor plots at zero
  trt_meds <- rbind(data.frame(trt="Compound", dose=as.numeric(as.character(unique(trt_meds[,"dose"]))), DIV=0, med=0, mad=0, n=1, low=0, high=0), trt_meds)
  trt_meds[,2:ncol(trt_meds)] <- lapply(trt_meds[,2:ncol(trt_meds)], as.numeric)
  
  # Isolate control standard deviations
  trt_cntrls <- subset(trt_meds, trt=="Control")
  trt_cntrls <- rbind(data.frame(trt="Control", dose=0, DIV=0, med=0, mad=0, n=1, low=0, high=0), trt_cntrls)
  
  # Repeat control values same number of times as total number of samples
  trt_cntrls2 <- trt_cntrls[rep(seq_len(nrow(trt_cntrls)), nrow(trt_meds)/nrow(trt_cntrls)),]
  
  # Change low and high values to two times the median absolute deviation. Update to 2*BMAD for BrainSpheres to follow tcplfit2 cut offs
  trt_cntrls2[,"low"] <- trt_cntrls2[,"med"] - 2*trt_cntrls2[,"mad"]
  trt_cntrls2[,"high"] <- trt_cntrls2[,"med"] + 2*trt_cntrls2[,"mad"]
  
  # Prepare SEM intervals
  limits <- aes(ymin=trt_meds[,"low"], ymax=trt_meds[,"high"])
  
  #trt_cntrls2$low
 #trt_cntrls2$high
  
  
 # Plot with ggplot
  ggplot(trt_meds, aes(x=DIV, y=med, group=dose, color=log10(dose+1))) +
    geom_ribbon(data=trt_cntrls2, aes(ymin=low, ymax=high, x=DIV), fill="grey70", alpha=0.8, inherit.aes=FALSE) + 
    geom_line(size=0.4, position=position_jitter(w=0, h=0.1)) +
    theme_minimal()+
    # geom_errorbar(limits, size=0.2, width=0.25) +  # Uncomment to add SEM error bars
    scale_color_gradient(low="blue", high="red") + 
    coord_cartesian(ylim=c(-10,25)) + 
    theme(axis.text=element_text(size=4), axis.title.x=element_text(size=6), axis.title.y=element_text(size=0), plot.title=element_text(size=6), legend.text=element_text(size=2), legend.position = "none") +
    ggtitle(paste(compound,parameter,sep="_"))
}

```


#Function to make multiple plots
```{r}
## Multiple plot function (sourced from http://peterhaschke.com/Code/multiplot.R)
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

#Function to plot all parameters together for a give compound
```{r}
## Plot all parameters together for a given compound
make_multi <- function(compound) {
  
  p <- list() #initialize plot list
  i <- 1 # intialize counter
  
  # loop through all network parameters
  for (j in names(all_data[8:(ncol(all_data))])){
    
    p[[i]] <- plot_dose_perturbations(z_norm_data, j, compound) # generate plot for each parameter
    i <- i + 1
  }
  
  # write multiplot to pdf file
  pdf(paste(compound, "_endpoints.pdf", sep=""))
  multiplot(plotlist=p, cols=4)
  dev.off() 
  
}
```

#Apply Muake Multi Function
```{r}
if (set_output == "zscore_plots") {
  
  # loop through each compound in dataset
  for (c in unique(all_data$trt)) {
    make_multi(c)
  } 
}
```


#Calculate Scalar perturbations and derivatives
```{r}
#Unable to run in a for loop- need to analyze each chemical separately
## Calculate scalar measure of total network parameter perturbation for EACH REPLICATE


Compounds<-"Glyphosate" #update for compound of interest

  # Subset normalized data to compound of interest
  comp_norm <- subset(z_norm_data, trt==Compounds)
  
  # Split compound data by date, plate ID, and well to separate replicates
  cnd_reps <- split(comp_norm, interaction(comp_norm[,"date"], comp_norm[,"Plate.SN"], comp_norm[,"well"], drop=TRUE))
  
  # Loop through replicates to compute Euclidean norm
  trt_sums <- list()
  trt_derivs <- list()
  n <- 1
  for (i in cnd_reps) {
    
    
set_parameters <- c("Neuron_Conduction_Velocity_Neuron","Firing_Rate_Neuron","Number_of_Spikes_per_Neuron_Neuron") 

    # Subset to set_parameters
    cnd_reps_sp <- cbind(i[,c("date","Plate.SN","DIV","well","trt","dose","units")], i[,set_parameters])
    
    # Calculate Euclidean norm across rows
    cnd_reps_sp[,"sum"] <- apply(cnd_reps_sp[,8:ncol(cnd_reps_sp)], 1, function(x) sqrt(sum(x^2)))
    
    # Add data frame to list
    trt_sums[[n]] <- cnd_reps_sp 
    
    # Isolate sums and calculate slopes
    trt_slopes <- cnd_reps_sp[,c("date","Plate.SN","DIV","well","trt","dose","sum")]
    trt_slopes[,"div15to20"] <- (subset(trt_slopes, DIV==20)[,"sum"][1] - subset(trt_slopes, DIV==15)[,"sum"][1])/5
    trt_slopes[,"div20to22"] <- (subset(trt_slopes, DIV==22)[,"sum"][1] - subset(trt_slopes, DIV==20)[,"sum"][1])/2
    trt_slopes[,"div22to27"] <- (subset(trt_slopes, DIV==27)[,"sum"][1] - subset(trt_slopes, DIV==22)[,"sum"][1])/5
    
    
    
    
    trt_slopes <- trt_slopes[nrow(trt_slopes),]
    
    # Add data frame of slopes to second list
    trt_derivs[[n]] <- trt_slopes   
    
    # Increment n
    n <- n + 1
  }  
  
  # Convert list of replicates to one dataframe
  trt_sums <- do.call(rbind, trt_sums)
  trt_derivs <- do.call(rbind, trt_derivs)
  
  # Subset to needed data only
  trt_sums <- trt_sums[,c("date","Plate.SN","DIV","well","trt","dose","sum")]
  
  # Add in DIV0 placeholder data to anchor plots at zero
  trt_sums <- rbind(data.frame(date=0, Plate.SN=0, DIV=0, well="AO", trt="Compounds", dose=as.numeric(as.character(unique(trt_sums[,"dose"]))), sum=0), trt_sums)
  #print(trt_sums)
  
  
  
  ## Now find 2*MAD variability of control wells
  # Get control well data across experiment
  cntrl_norm <- subset(z_norm_data, trt=="Control")
  
  # Split compound data by date, plate ID, and well to separate replicates
  cntrl_reps <- split(cntrl_norm, interaction(cntrl_norm[,"date"], cntrl_norm[,"Plate.SN"], cntrl_norm[,"well"], drop=TRUE))
  
  # Loop through control well replicates to compute Euclidean norm
  cntrl_sums <- list()
  n <- 1
  for (i in cntrl_reps) {
    
    # Subset to set_parameters
    cntrl_reps_sp <- cbind(i[,c("date","Plate.SN","DIV","well","trt","dose","units")], i[,set_parameters])
    
    # Calculate Euclidean norm across rows
    cntrl_reps_sp[,"sum"] <- apply(cntrl_reps_sp[,8:ncol(cntrl_reps_sp)], 1, function(x) sqrt(sum(x^2)))
    
    # Add data frame to list
    cntrl_sums[[n]] <- cntrl_reps_sp 
    n <- n + 1
  } 
  
  # Convert list of replicates to one dataframe
  cntrl_sums <- do.call(rbind, cntrl_sums)
  
  # Aggregate to find 2*MAD across DIV timepoints
  #Update to 2*BMAD for BrainSpheres to follow tcplfit2 cut offs
  cntrl_var <- as.data.frame(do.call(cbind, aggregate(sum ~ DIV, FUN=function(x) c(med=median(x), mad=2*mad(x), n=length(x)), data=cntrl_sums)), stringsAsFactors=FALSE)
  
  # Add min and max values for plotting
  cntrl_var[,"min"] <- 0
  cntrl_var[,"max"] <- cntrl_var[,"mad"] + cntrl_var[,"med"] # adding the 3*MAD to the median perturbation of controls
  
  # Add in DIV0 placeholder value 
  cntrl_var <- rbind(data.frame(DIV=0, med=0, mad=0, n=1, min=0, max=0), cntrl_var)
  
  # Get DIV12 3*MAD for derivative plotting
  final_var <- subset(cntrl_var, DIV==27)[,"max"][1]
  
    ## Plot scalar perturbations with ggplot2
  vis <- ggplot(trt_sums, aes(x=DIV, y=sum, group=dose, color=log10(dose))) +
    theme_minimal() +
    geom_ribbon(data=cntrl_var, aes(ymin=cntrl_var[,"min"], ymax=cntrl_var[,"max"], x=cntrl_var[,"DIV"]), fill="grey70", alpha=0.8, linetype=2, inherit.aes=FALSE) +
    geom_smooth(method=loess,formula = y ~ x,size=1.4, se=FALSE, span=1) +
    #geom_point(position=position_jitter(w=0.25, h=0.2)) +
    scale_color_gradient(low="blue", high="red") +
    coord_cartesian(ylim=c(0,20)) +
    #ggtitle(Compounds) +
    scale_x_continuous(breaks = c(0,2,5,7,9,12,14,16,19,21,23,26)) +
    theme(axis.text=element_text(size=12), axis.title.x=element_text(size=12), axis.title.y=element_text(size=0), plot.title=element_text(size=12), legend.text=element_text(size=12))+
  theme(legend.position = "none")
  vis
  

ggsave(vis,filename=paste(Compounds,"_Total_Scalar_Perturbations_2",".png",sep=""),height=5.01,width=5.32,unit="in")


  
   vis15_20 <- ggplot(trt_derivs, aes(x=log10(dose))) +
    theme_minimal() +
    geom_hline(yintercept=final_var, linetype=2) +
    geom_hline(yintercept=0, linetype=2) +
    geom_smooth(aes(y=sum), size=1.4, se=TRUE, span=0.8, col="blue", fill="blue") +
    geom_smooth(aes(y=div15to20*5), size=1.4, se=TRUE, span=0.8, col="forestgreen", fill="forestgreen") +
    geom_point(aes(y=sum), col="blue", position=position_jitter(w=0.2, h=0.25)) +
    geom_point(aes(y=div15to20*5), col="forestgreen", position=position_jitter(w=0.2, h=0.25)) +
    coord_cartesian(ylim=c(-50,50)) +
    ggtitle("DIV15-20") +
    theme(axis.text=element_text(size=12), axis.title.x=element_text(size=12), axis.title.y=element_text(size=0),   plot.title=element_text(size=22), legend.text=element_text(size=12))+
    theme(plot.title = element_text(hjust = 0.5))   
  vis15_20
  
    vis20_22 <- ggplot(trt_derivs, aes(x=log10(dose))) +
    theme_minimal() +
    geom_hline(yintercept=final_var, linetype=2) +
    geom_hline(yintercept=0, linetype=2) +
    geom_smooth(aes(y=sum), size=1.4, se=TRUE, span=0.8, col="blue", fill="blue") +
    geom_smooth(aes(y=div20to22*5), size=1.4, se=TRUE, span=0.8, col="forestgreen", fill="forestgreen") +
    geom_point(aes(y=sum), col="blue", position=position_jitter(w=0.2, h=0.25)) +
    geom_point(aes(y=div20to22*5), col="forestgreen", position=position_jitter(w=0.2, h=0.25)) +
    coord_cartesian(ylim=c(-50,50)) +
    ggtitle("DIV20-22") +
    theme(axis.text=element_text(size=12), axis.title.x=element_text(size=12), axis.title.y=element_text(size=0),   plot.title=element_text(size=22), legend.text=element_text(size=12))+
    theme(plot.title = element_text(hjust = 0.5))  
  
  
    vis22_27 <- ggplot(trt_derivs, aes(x=log10(dose))) +
    theme_minimal() +
    geom_hline(yintercept=final_var, linetype=2) +
    geom_hline(yintercept=0, linetype=2) +
    geom_smooth(aes(y=sum), size=1.4, se=TRUE, span=0.8, col="blue", fill="blue") +
    geom_smooth(aes(y=div22to27*5), size=1.4, se=TRUE, span=0.8, col="forestgreen", fill="forestgreen") +
    geom_point(aes(y=sum), col="blue", position=position_jitter(w=0.2, h=0.25)) +
    geom_point(aes(y=div22to27*5), col="forestgreen", position=position_jitter(w=0.2, h=0.25)) +
    coord_cartesian(ylim=c(-50,50)) +
    ggtitle("DIV22-27") +
    theme(axis.text=element_text(size=12), axis.title.x=element_text(size=12), axis.title.y=element_text(size=0),   plot.title=element_text(size=22), legend.text=element_text(size=12))+
    theme(plot.title = element_text(hjust = 0.5)) 
  vis22_27
    # Find derivative of slope function
  add_deriv <- function(dv_plot, timeframe) {

    
    # Get change in scalar over time loess fitted values
    # The fourth function in the vis2-4 ggplot calls is the slope
    x_vals <- ggplot_build(dv_plot)$data[[4]]$x
    y_vals <- ggplot_build(dv_plot)$data[[4]]$y
    y_mins <- ggplot_build(dv_plot)$data[[4]]$ymin
    y_maxs <- ggplot_build(dv_plot)$data[[4]]$ymax
    
    # Approximate derivative with change in y over change in x
    xy_prime <- diff(y_vals)/diff(x_vals)
    xy_prime_min <- diff(y_mins)/diff(x_vals)
    xy_prime_max <- diff(y_maxs)/diff(x_vals)
    deriv <- data.frame(x=x_vals[2:length(x_vals)], y=xy_prime, ymin=xy_prime_min, ymax=xy_prime_max)
    
    # Add derivative to ggplot object
    dv_plot <- dv_plot + geom_smooth(aes(x=x, y=y), data=deriv, inherit.aes=FALSE, size=1.4, se=FALSE, span=0.8, col="red", fill="red") +
      #geom_ribbon(aes(x=x, ymax=ymax, ymin=ymin), data=deriv, inherit.aes=FALSE, alpha=0.5, fill="red")
      geom_smooth(aes(x=x, y=ymin), data=deriv, inherit.aes=FALSE, size=0.8, se=FALSE, span=0.8, col="red", fill="red") +
      geom_smooth(aes(x=x, y=ymax), data=deriv, inherit.aes=FALSE, size=0.8, se=FALSE, span=0.8, col="red", fill="red")
    
    # Get derivative loess fitted values to remove noise in derivative function
    # The 7th function in the vis2-4 ggplot calls is now the derivative (slope change over concentration change)
    x_vals <- ggplot_build(dv_plot)$data[[7]]$x
    y_vals <- ggplot_build(dv_plot)$data[[7]]$y
    y_mins <- ggplot_build(dv_plot)$data[[8]]$y
    y_maxs <- ggplot_build(dv_plot)$data[[9]]$y
    
    # Replace derivative with smoothened values
    deriv <- data.frame(x=x_vals, y=y_vals, ymin=y_mins, ymax=y_maxs)
    

    # If low end of confidence interval for fitted scalar perturb at highest dose exceeds 3*MAD of controls,
    # And at least 5 y-values of the velocity curve fit are negative (recovering trajectory)
    # And at least one y-value of the derivative is distinctly positive and one y-value is negative,
    # Find first dose (x-value) where derivative crosses y=0 line
    if (max(ggplot_build(dv_plot)$data[[3]]$ymin[75:80]) > final_var && sort(ggplot_build(dv_plot)$data[[4]]$ymin[1:40])[5] < 0 && max(deriv[,"y"]) > 0.001 && min(deriv[1:40,"y"]) < -0.001) {
      
      # If the highest dose derivative values are negative, remove them 
      # This represents topping out of scalar perturbation, which is not of interest 
      # Use cutoff slightly greater than zero to also catch practically zero slopes
      if (deriv[nrow(deriv),"y"] < 0.001) {
        last_pos <- max(which(deriv[,"y"] > 0.001)) # index of last positive value
        deriv <- deriv[1:last_pos,] # subset to remove last string of negative values
      }
      
      # If there is still a negative derivative value, find index of last negative and estimate tipping point
      if (min(deriv[,"y"]) < 0) {
        last_neg <- max(which(deriv[,"y"] < 0)) # index of last negative value
        root1 <- round(10^(deriv[last_neg+1,"x"]), 3) # Get dose of next (positive) value
        dv_plot <- dv_plot + geom_vline(xintercept=log10(root1), linetype=2) # Add tipping point estimate to ggplot object
        
        # If root value is at lower concentration than 50% cytotox dose, report it
        if (root1 < 100) {
          cat(paste(Compounds, root1, sep=','))
        } else { 
          cat(paste(Compounds, "NA", "NA", "NA", sep=','), "\n")
        }
         

      # Otherwise, the first dose produced a positive slope, so set the tipping point to undefined
      } else {
        root1 <- NA
        cat(paste(Compounds, root1, "NA", "NA", sep=','), "\n")
      }
    
    # Else, we could not define a tipping point concentration
    } else {
      root1 <- NA
      cat(paste(Compounds, root1, "NA", "NA", sep=','), "\n")
    }
    
  
      
    
    if (!is.na(root1)) {
      ## Estimating critical concentration error by fitting loess models to randomly sampled data subsets
      trt_deriv_split <- split(trt_derivs, trt_derivs[,"dose"]) # split up slope values by dose tested
      
      samps <- c() # intialize vector of roots
      while(length(na.omit(samps)) < 1000) { #loop through sampling 1000 times
        
        smpld_trt_deriv <- list() # intialize sample list
        n <- 1 # initialize counter
        for (i in trt_deriv_split){ #loop through each concentration
          smpld_trt_deriv[[n]] <- i[sample(nrow(i), nrow(i)-1),] # sample one less than number of rows (replicates) from each concentration tested
          n <- n+1
        }
    
        smpld_trt_deriv <- do.call(rbind, smpld_trt_deriv) # form sample data frame
        
        timeframe_loess <- loess(get(timeframe) ~ log10(dose), smpld_trt_deriv, span=0.8) # fit localized regression
        pred_range <- seq(from=min(log10(smpld_trt_deriv[,"dose"])), to=max(log10(smpld_trt_deriv[,"dose"])), length.out=80) # use 80 x-values to predict y-values for the model (matching geom_smooth)
        vel_curve <- 5*predict(timeframe_loess, pred_range) # output model y value predictions
        
        xy_prime <- diff(vel_curve)/diff(pred_range) # approximate derivative by taking change in velocity curve over change in concentration
        deriv <- data.frame(x=pred_range[2:length(pred_range)], y=xy_prime)
        deriv_loess <- loess(y ~ x, deriv, span=0.8) # model derivative with loess smoothing
        deriv_curve <- predict(deriv_loess, pred_range[2:length(pred_range)]) # get smoothened derivative
        deriv <- data.frame(x=pred_range[2:length(pred_range)], y=deriv_curve) # replace df with fitted values
        
        # make sure the derivative fit crosses y=0
        if (max(deriv[,"y"]) > 0.001 && min(deriv[,"y"]) < -0.001) {
          
          # If the highest dose derivative values are negative, remove them 
          # This represents topping out of scalar perturbation, which is not of interest 
          # Use cutoff slightly greater than zero to also catch practically zero slopes
          if (deriv[nrow(deriv),"y"] < 0.001) {
            last_pos <- max(which(deriv[,"y"] > 0.001)) # index of last positive value
            deriv <- deriv[1:last_pos,] # subset to remove last string of negative values
          }
          
          # If there is still a negative derivative value, find index of last negative and estimate tipping point
          if (min(deriv[,"y"]) < 0) {
            last_neg <- max(which(deriv[,"y"] < 0)) # index of last negative value
            samp_root <- round(10^(deriv[last_neg+1,"x"]), 3) # Get dose of next (positive) value
          } else {
            samp_root <- NA
          } 
          
          # else, the derivative fit did not cross y=0, set to NA  
        } else { 
          samp_root <- NA
        }
        
        samps <- append(samps, samp_root) # add sampled data root to growing vector 
        }
      
      #print(sort(na.omit(samps)))
      #print(quantile(na.omit(samps), probs=c(0.025, 0.975)))
      ci_95 <- quantile(na.omit(samps), probs=c(0.025, 0.975)) # Get 95% CI from 1000 sampled roots
      ci_low <- round(ci_95[[1]], 3)
      ci_hi <- round(ci_95[[2]], 3)
      cat(",", paste(ci_low, ci_hi, sep=","), "\n", sep="") # Report 95% CI limits
    }  
  
      # Print plot      
      print(dv_plot)
    }
  
  # write results to .csv file
sink(file=paste(Compounds,"_Tipping_Point_List",".csv",sep=""), append=TRUE, split=TRUE)

add_deriv(vis15_20, "div15to20")
add_deriv(vis20_22, "div20to22")
add_deriv(vis22_27, "div22to27")

  sink()
    

 vis15_20<- add_deriv(vis15_20, "div15to20")
 vis20_22<- add_deriv(vis20_22, "div20to22")
 vis22_27<- add_deriv(vis22_27, "div22to27")



library(ggpubr)

vis_plot_list<-list(vis15_20,vis20_22,vis22_27)

vis_plots<-ggarrange(plotlist=vis_plot_list,ncol=3,nrow=3)
vis_plots

#ggsave
ggsave(plot=vis_plots,filename=paste(Compounds,"_Tipping_Point_Determination_2",".png",sep=""),bg="white",width=17,height=17,units="in")

while (!is.null(dev.list()))  dev.off()

```









